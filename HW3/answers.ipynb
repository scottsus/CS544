{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/8jk4g7fx3xl9p5mzpl0rkqj40000gp/T/ipykernel_85269/3029288783.py:4: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, delimiter='\\t', on_bad_lines='skip')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>43081963</td>\n",
       "      <td>R18RVCKGH1SSI9</td>\n",
       "      <td>B001BM2MAC</td>\n",
       "      <td>307809868</td>\n",
       "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>Great product.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10951564</td>\n",
       "      <td>R3L4L6LW1PUOFY</td>\n",
       "      <td>B00DZYEXPQ</td>\n",
       "      <td>75004341</td>\n",
       "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
       "      <td>What's to say about this commodity item except...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>21143145</td>\n",
       "      <td>R2J8AWXWTDX2TF</td>\n",
       "      <td>B00RTMUHDW</td>\n",
       "      <td>529689027</td>\n",
       "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>but I am sure I will like it.</td>\n",
       "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>52782374</td>\n",
       "      <td>R1PR37BR7G3M6A</td>\n",
       "      <td>B00D7H8XB6</td>\n",
       "      <td>868449945</td>\n",
       "      <td>AmazonBasics 12-Sheet High-Security Micro-Cut ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>and the shredder was dirty and the bin was par...</td>\n",
       "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>24045652</td>\n",
       "      <td>R3BDDDZMZBZDPU</td>\n",
       "      <td>B001XCWP34</td>\n",
       "      <td>33521401</td>\n",
       "      <td>Derwent Colored Pencils, Inktense Ink Pencils,...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>Gorgeous colors and easy to use</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640249</th>\n",
       "      <td>US</td>\n",
       "      <td>53005790</td>\n",
       "      <td>RLI7EI10S7SN0</td>\n",
       "      <td>B00000DM9M</td>\n",
       "      <td>223408988</td>\n",
       "      <td>PalmOne III Leather Belt Clip Case</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>26.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Great value! A must if you hate to carry thing...</td>\n",
       "      <td>I can't live anymore whithout my Palm III. But...</td>\n",
       "      <td>1998-12-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640250</th>\n",
       "      <td>US</td>\n",
       "      <td>52188548</td>\n",
       "      <td>R1F3SRK9MHE6A3</td>\n",
       "      <td>B00000DM9M</td>\n",
       "      <td>223408988</td>\n",
       "      <td>PalmOne III Leather Belt Clip Case</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Attaches the Palm Pilot like an appendage</td>\n",
       "      <td>Although the Palm Pilot is thin and compact it...</td>\n",
       "      <td>1998-11-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640251</th>\n",
       "      <td>US</td>\n",
       "      <td>52090046</td>\n",
       "      <td>R23V0C4NRJL8EM</td>\n",
       "      <td>0807865001</td>\n",
       "      <td>307284585</td>\n",
       "      <td>Gods and Heroes of Ancient Greece</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Excellent information, pictures and stories, I...</td>\n",
       "      <td>This book had a lot of great content without b...</td>\n",
       "      <td>1998-10-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640252</th>\n",
       "      <td>US</td>\n",
       "      <td>52503173</td>\n",
       "      <td>R13ZAE1ATEUC1T</td>\n",
       "      <td>1572313188</td>\n",
       "      <td>870359649</td>\n",
       "      <td>Microsoft EXCEL 97/ Visual Basic Step-by-Step ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>class text</td>\n",
       "      <td>I am teaching a course in Excel and am using t...</td>\n",
       "      <td>1998-08-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2640253</th>\n",
       "      <td>US</td>\n",
       "      <td>52585611</td>\n",
       "      <td>RE8J5O2GY04NN</td>\n",
       "      <td>1572313188</td>\n",
       "      <td>870359649</td>\n",
       "      <td>Microsoft EXCEL 97/ Visual Basic Step-by-Step ...</td>\n",
       "      <td>Office Products</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Microsoft's Finest</td>\n",
       "      <td>A very comprehensive layout of exactly how Vis...</td>\n",
       "      <td>1998-07-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640021 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0                US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
       "1                US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
       "2                US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
       "3                US     52782374  R1PR37BR7G3M6A  B00D7H8XB6       868449945   \n",
       "4                US     24045652  R3BDDDZMZBZDPU  B001XCWP34        33521401   \n",
       "...             ...          ...             ...         ...             ...   \n",
       "2640249          US     53005790   RLI7EI10S7SN0  B00000DM9M       223408988   \n",
       "2640250          US     52188548  R1F3SRK9MHE6A3  B00000DM9M       223408988   \n",
       "2640251          US     52090046  R23V0C4NRJL8EM  0807865001       307284585   \n",
       "2640252          US     52503173  R13ZAE1ATEUC1T  1572313188       870359649   \n",
       "2640253          US     52585611   RE8J5O2GY04NN  1572313188       870359649   \n",
       "\n",
       "                                             product_title product_category  \\\n",
       "0           Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
       "1                Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
       "2        Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
       "3        AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
       "4        Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
       "...                                                    ...              ...   \n",
       "2640249                 PalmOne III Leather Belt Clip Case  Office Products   \n",
       "2640250                 PalmOne III Leather Belt Clip Case  Office Products   \n",
       "2640251                  Gods and Heroes of Ancient Greece  Office Products   \n",
       "2640252  Microsoft EXCEL 97/ Visual Basic Step-by-Step ...  Office Products   \n",
       "2640253  Microsoft EXCEL 97/ Visual Basic Step-by-Step ...  Office Products   \n",
       "\n",
       "        star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0                 5            0.0          0.0    N                 Y   \n",
       "1                 5            0.0          1.0    N                 Y   \n",
       "2                 5            0.0          0.0    N                 Y   \n",
       "3                 1            2.0          3.0    N                 Y   \n",
       "4                 4            0.0          0.0    N                 Y   \n",
       "...             ...            ...          ...  ...               ...   \n",
       "2640249           4           26.0         26.0    N                 N   \n",
       "2640250           4           18.0         18.0    N                 N   \n",
       "2640251           4            9.0         16.0    N                 N   \n",
       "2640252           5            0.0          0.0    N                 N   \n",
       "2640253           5            0.0          0.0    N                 N   \n",
       "\n",
       "                                           review_headline  \\\n",
       "0                                               Five Stars   \n",
       "1        Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
       "2                            but I am sure I will like it.   \n",
       "3        and the shredder was dirty and the bin was par...   \n",
       "4                                               Four Stars   \n",
       "...                                                    ...   \n",
       "2640249  Great value! A must if you hate to carry thing...   \n",
       "2640250          Attaches the Palm Pilot like an appendage   \n",
       "2640251  Excellent information, pictures and stories, I...   \n",
       "2640252                                         class text   \n",
       "2640253                                 Microsoft's Finest   \n",
       "\n",
       "                                               review_body review_date  \n",
       "0                                           Great product.  2015-08-31  \n",
       "1        What's to say about this commodity item except...  2015-08-31  \n",
       "2          Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
       "3        Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
       "4                          Gorgeous colors and easy to use  2015-08-31  \n",
       "...                                                    ...         ...  \n",
       "2640249  I can't live anymore whithout my Palm III. But...  1998-12-07  \n",
       "2640250  Although the Palm Pilot is thin and compact it...  1998-11-30  \n",
       "2640251  This book had a lot of great content without b...  1998-10-15  \n",
       "2640252  I am teaching a course in Excel and am using t...  1998-08-22  \n",
       "2640253  A very comprehensive layout of exactly how Vis...  1998-07-15  \n",
       "\n",
       "[2640021 rows x 15 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = './amazon_reviews_us_Office_Products_v1_00.tsv'\n",
    "df = pd.read_csv(file_path, delimiter='\\t', on_bad_lines='skip')\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form classes and sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['review_body', 'star_rating']\n",
    "df = df[cols]\n",
    "\n",
    "df['review_body'] = df['review_body'].apply(lambda review: '' if pd.isna(review) or isinstance(review, float) else review)\n",
    "\n",
    "class_1 = df[df['star_rating'].isin([1, 2, 3])]\n",
    "class_2 = df[df['star_rating'].isin([4, 5])]\n",
    "\n",
    "sample_size = 50_000\n",
    "class_1 = class_1.sample(sample_size, random_state=42)\n",
    "class_2 = class_2.sample(sample_size, random_state=42)\n",
    "\n",
    "balanced_df = pd.concat([class_1, class_2])\n",
    "balanced_df.loc[:, 'class'] = balanced_df['star_rating'].apply(lambda x: 1 if x in [1, 2, 3] else 2 if x in [4, 5] else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download gensim Google Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as downloader\n",
    "\n",
    "wv = downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a basic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman = queen\n",
      "france - paris + berlin = germany\n",
      "coffee - beans + leaves = leaving\n"
     ]
    }
   ],
   "source": [
    "def basic_test(model):\n",
    "    examples = [\n",
    "        ('king', 'man', 'woman'),\n",
    "        ('france', 'paris', 'berlin'),\n",
    "        ('coffee', 'beans', 'leaves'),\n",
    "    ]\n",
    "\n",
    "    for (word1, word2, word3) in examples:\n",
    "        try:\n",
    "            result = model.most_similar(positive=[word1, word3], negative=[word2], topn=1)\n",
    "            print(f'{word1} - {word2} + {word3} = {result[0][0]}')\n",
    "        except KeyError:\n",
    "            print(f'\"{word1}\", \"{word2}\", or \"{word3}\" does not exist in this model')\n",
    "\n",
    "basic_test(wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus for training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that processes each line in dataset.\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        data = balanced_df['review_body']\n",
    "        for line in data:\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences, vector_size=300, window=13, min_count=9, workers=4)\n",
    "custom_model = model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman = openers\n",
      "\"france\", \"paris\", or \"berlin\" does not exist in this model\n",
      "coffee - beans + leaves = surface\n"
     ]
    }
   ],
   "source": [
    "basic_test(custom_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom vs pretrained model analysis\n",
    "\n",
    "We conclude that my model is much less precise than the pretrained Google model. For example, king - man + woman = oversize, which has no semantic similarity at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Simple Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = balanced_df['review_body']\n",
    "y = balanced_df['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define average Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_average_w2v(review_string, model, k=300):\n",
    "    review = review_string.split(' ')\n",
    "    if (len(review) < 1):\n",
    "        return np.zeros(k)\n",
    "    \n",
    "    embeddings = np.zeros(k)\n",
    "    for word in review:\n",
    "        try:\n",
    "            embeddings += model[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    average_embedding = np.divide(embeddings, len(embeddings))\n",
    "\n",
    "    return average_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test set using Google model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_model = wv\n",
    "\n",
    "X_train_w2v = X_train.apply(lambda review: get_average_w2v(review, google_model))\n",
    "X_test_w2v = X_test.apply(lambda review: get_average_w2v(review, google_model))\n",
    "\n",
    "X_train_w2v = np.array(X_train_w2v.tolist())\n",
    "X_test_w2v = np.array(X_test_w2v.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create test set using TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron accuracy with Word2Vec: 0.7965\n",
      "Perceptron accuracy with TF-IDF: 0.82425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "perc_w2v = Perceptron()\n",
    "perc_w2v.fit(X_train_w2v, y_train)\n",
    "y_pred_perc_w2v = perc_w2v.predict(X_test_w2v)\n",
    "accuracy_perc_w2v = accuracy_score(y_test, y_pred_perc_w2v)\n",
    "\n",
    "perc_tfidf = Perceptron()\n",
    "perc_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_perc_tfidf = perc_tfidf.predict(X_test_tfidf)\n",
    "accuracy_perc_tfidf = accuracy_score(y_test, y_pred_perc_tfidf)\n",
    "\n",
    "print(f'Perceptron accuracy with Word2Vec: {accuracy_perc_w2v}')\n",
    "print(f'Perceptron accuracy with TF-IDF: {accuracy_perc_tfidf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM accuracy with Word2Vec: 0.78975\n",
      "SVM accuracy with TF-IDF: 0.8649\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_w2v = LinearSVC(dual='auto')\n",
    "svm_w2v.fit(X_train_w2v, y_train)\n",
    "y_pred_svm_w2v = svm_w2v.predict(X_test_w2v)\n",
    "accuracy_svm_w2v = accuracy_score(y_test, y_pred_svm_w2v)\n",
    "\n",
    "svm_tfidf = LinearSVC(dual='auto')\n",
    "svm_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
    "accuracy_svm_tfidf = accuracy_score(y_test, y_pred_svm_tfidf)\n",
    "\n",
    "print(f'SVM accuracy with Word2Vec: {accuracy_svm_w2v}')\n",
    "print(f'SVM accuracy with TF-IDF: {accuracy_svm_tfidf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Neural Network\n",
    "Our entry point will be the function `evaluate_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(300, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "def train_model(model, train_loader):\n",
    "    lr = 1e-3\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        for reviews, labels in train_loader:\n",
    "            outputs = model(reviews)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_loader, description):\n",
    "    with torch.no_grad():\n",
    "        for reviews, labels in test_loader:\n",
    "            outputs = model(reviews)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "            accuracy = (pred == labels).sum().item() / labels.size(0)\n",
    "\n",
    "    print(f'Accuracy with {description}: {accuracy * 100:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper function to generate loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def get_train_test_splits(get_w2v):\n",
    "    X_train_local = X_train.apply(lambda review: get_w2v(review, google_model))\n",
    "    X_test_local = X_test.apply(lambda review: get_w2v(review, google_model))\n",
    "\n",
    "    X_train_concat = np.array(X_train_local.values.tolist())\n",
    "    X_test_concat = np.array(X_test_local.values.tolist())\n",
    "\n",
    "    return X_train_concat, X_test_concat\n",
    "\n",
    "def create_loader_from_train_test_splits(X_train, X_test):\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.LongTensor(y_train.values)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    y_test_tensor = torch.LongTensor(y_test.values)\n",
    "\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def create_loader(get_w2v):\n",
    "    X_train_concat, X_test_concat = get_train_test_splits(get_w2v)\n",
    "    train_loader, test_loader = create_loader_from_train_test_splits(X_train_concat, X_test_concat)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build train & test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_a, test_loader_a = create_loader(get_average_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & evaluate `Average Word2Vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4977\n",
      "Epoch 2/10, Loss: 0.5192\n",
      "Epoch 3/10, Loss: 0.4365\n",
      "Epoch 4/10, Loss: 0.5107\n",
      "Epoch 5/10, Loss: 0.5178\n",
      "Epoch 6/10, Loss: 0.5297\n",
      "Epoch 7/10, Loss: 0.4925\n",
      "Epoch 8/10, Loss: 0.5193\n",
      "Epoch 9/10, Loss: 0.4826\n",
      "Epoch 10/10, Loss: 0.5251\n",
      "Accuracy with Average Word2Vec: 84.3750\n"
     ]
    }
   ],
   "source": [
    "model_a = train_model(MLP(), train_loader_a)\n",
    "evaluate_model(model_a, test_loader_a, 'Average Word2Vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenated Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concatenated_w2v(review_string, model, k=300):\n",
    "    review = review_string.split(' ')\n",
    "    if (len(review) < 1):\n",
    "        return np.zeros(k)\n",
    "    \n",
    "    embeddings = np.zeros(k)\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            word = review[i]\n",
    "            embeddings += model[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            break\n",
    "    \n",
    "    average_embedding = np.divide(embeddings, len(embeddings))\n",
    "\n",
    "    return average_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build train & test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_b, test_loader_b = create_loader(get_concatenated_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate `Concatenated Word2Vec`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5884\n",
      "Epoch 2/10, Loss: 0.6318\n",
      "Epoch 3/10, Loss: 0.6177\n",
      "Epoch 4/10, Loss: 0.5616\n",
      "Epoch 5/10, Loss: 0.5781\n",
      "Epoch 6/10, Loss: 0.5250\n",
      "Epoch 7/10, Loss: 0.5094\n",
      "Epoch 8/10, Loss: 0.5775\n",
      "Epoch 9/10, Loss: 0.5938\n",
      "Epoch 10/10, Loss: 0.5785\n",
      "Accuracy with Concatenated Word2Vec: 65.6250\n"
     ]
    }
   ],
   "source": [
    "model_b = train_model(MLP(), train_loader_b)\n",
    "evaluate_model(model_b, test_loader_b, 'Concatenated Word2Vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average vs Concatenated Word2Vec Analysis\n",
    "\n",
    "Intuitively, taking only the first 10 vectors would be more lossy than averaging all the vectors in the review, so naturally test data shows that we have a much lower accuracy of about `62% in Concatenated Word2Vec` compared to about `81% in Average Word2Vec`.\n",
    "\n",
    "We conclude that the difference in precision is too large for us to effectively use `Concatenated Word2Vec`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.input_dim = 300\n",
    "        self.hidden_dim = 10\n",
    "        self.output_dim = 2\n",
    "        self.rnn = nn.RNN(self.input_dim, self.hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_dim)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "def get_2d_w2v(review_string, model, k=300):\n",
    "    review = review_string.split()\n",
    "    embeddings = np.zeros((10, k))\n",
    "\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            word = review[i]\n",
    "            embeddings[i] = model[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except IndexError:\n",
    "            break\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_rnn, test_loader_rnn = create_loader(get_2d_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.5107\n",
      "Epoch 2/10, Loss: 0.6250\n",
      "Epoch 3/10, Loss: 0.5401\n",
      "Epoch 4/10, Loss: 0.5747\n",
      "Epoch 5/10, Loss: 0.4220\n",
      "Epoch 6/10, Loss: 0.5225\n",
      "Epoch 7/10, Loss: 0.5083\n",
      "Epoch 8/10, Loss: 0.4677\n",
      "Epoch 9/10, Loss: 0.4700\n",
      "Epoch 10/10, Loss: 0.5705\n",
      "Accuracy with RNN: 84.3750\n"
     ]
    }
   ],
   "source": [
    "simple_rnn = train_model(SimpleRNN(), train_loader_rnn)\n",
    "evaluate_model(simple_rnn, test_loader_rnn, 'RNN')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HW3venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
