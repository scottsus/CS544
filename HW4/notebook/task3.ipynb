{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW4: Deep Learning on NER"
      ],
      "metadata": {
        "id": "GWP_sptpnvil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup environment"
      ],
      "metadata": {
        "id": "rWkxgZCsoem-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZiwf9icmUs6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7982bb-63e4-407e-cb15-752d829bf3b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/493.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.4/493.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q datasets accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE0bntfvnPeT",
        "outputId": "6c37e8c1-fb34-4ba4-b831-9d8dafb5ab5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-11 04:16:17--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-11-11 04:16:17--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-11-11 04:16:17--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: â€˜glove.6B.zipâ€™\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  3.89MB/s    in 2m 39s  \n",
            "\n",
            "2023-11-11 04:18:57 (5.16 MB/s) - â€˜glove.6B.zipâ€™ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-LZJDrnoKgS",
        "outputId": "71fda09c-15cb-44e1-b731-3573d445d74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOEhdzR4niKT",
        "outputId": "d64379fa-eb36-40a2-c801-e5a79d7ce3e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-11 04:19:32--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7502 (7.3K) [text/plain]\n",
            "Saving to: â€˜conlleval.pyâ€™\n",
            "\n",
            "conlleval.py        100%[===================>]   7.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-11-11 04:19:32 (33.3 MB/s) - â€˜conlleval.pyâ€™ saved [7502/7502]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls # Sanity check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDT6t_mJopVJ",
        "outputId": "a1210581-4ebc-4888-cc91-09cd4ed3d5a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conlleval.py\t   glove.6B.200d.txt  glove.6B.50d.txt\tsample_data\n",
            "glove.6B.100d.txt  glove.6B.300d.txt  glove.6B.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 0: Prepare Data"
      ],
      "metadata": {
        "id": "pCPgCp3CnmaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset"
      ],
      "metadata": {
        "id": "My5ffrs7n_wJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "\n",
        "dataset = datasets.load_dataset(\"conll2003\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_oEW-oInle6",
        "outputId": "310bf125-5cfc-4cb0-f21b-4b323986eb1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use GloVe embeddings"
      ],
      "metadata": {
        "id": "VuPw9xkGCM0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vocab, embeddings = [], []\n",
        "with open('glove.6B.100d.txt', 'rt') as glove_file:\n",
        "  full_content = glove_file.read().strip().split('\\n')\n",
        "\n",
        "for i in range(len(full_content)):\n",
        "  word = full_content[i].split(' ')[0]\n",
        "  embedding = [float(val) for val in full_content[i].split(' ')[1:]]\n",
        "  vocab.append(word)\n",
        "  embeddings.append(embedding)\n",
        "\n",
        "vocab_npa = np.array(vocab)\n",
        "embeddings_npa = np.array(embeddings)\n",
        "\n",
        "vocab_npa = np.insert(vocab_npa, 0, '[PAD]')\n",
        "vocab_npa = np.insert(vocab_npa, 1, '[UNK]')\n",
        "\n",
        "pad_embeddings_npa = np.zeros((1, embeddings_npa.shape[1]))\n",
        "unk_embeddings_npa = np.mean(embeddings_npa, axis=0, keepdims=True)\n",
        "\n",
        "embeddings_npa = np.vstack((pad_embeddings_npa, unk_embeddings_npa, embeddings_npa))\n",
        "embeddings_npa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD5yDqmYCL_P",
        "outputId": "a0a0bd86-72ea-482e-d2b3-f73eef75c2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
              "         0.        ,  0.        ],\n",
              "       [ 0.05209832, -0.09711439, -0.13807563, ...,  0.12381253,\n",
              "        -0.23434524, -0.00925516],\n",
              "       [-0.038194  , -0.24487   ,  0.72812   , ..., -0.1459    ,\n",
              "         0.8278    ,  0.27062   ],\n",
              "       ...,\n",
              "       [ 0.36088   , -0.16919   , -0.32704   , ...,  0.27139   ,\n",
              "        -0.29188   ,  0.16109   ],\n",
              "       [-0.10461   , -0.5047    , -0.49331   , ...,  0.42527   ,\n",
              "        -0.5125    , -0.17054   ],\n",
              "       [ 0.28365   , -0.6263    , -0.44351   , ...,  0.43678   ,\n",
              "        -0.82607   , -0.15701   ]])"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2idx = {\n",
        "    word.lower(): idx for idx, word in enumerate(vocab, start=2)\n",
        "}\n",
        "\n",
        "word2idx['[PAD]'] = 0\n",
        "word2idx['[UNK]'] = 1\n",
        "\n",
        "dataset = (\n",
        "    dataset.map(lambda x: {\n",
        "        'input_ids': [\n",
        "            word2idx.get(word.lower(), word2idx['[UNK]']) for word in x['tokens']\n",
        "        ]\n",
        "    })\n",
        ")\n",
        "\n",
        "dataset['train']['input_ids'][:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnT29THgqd_o",
        "outputId": "a5b3635a-e5eb-4acb-fc4a-cf8b362ff86b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[646, 7580, 516, 582, 6, 5262, 299, 10240, 4], [1296, 9005], [3881, 1]]"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.rename_column('ner_tags', 'labels')\n",
        "dataset = dataset.remove_columns(['pos_tags', 'chunk_tags'])\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOCh-MojrvZQ",
        "outputId": "64ed81ee-f857-4bcf-b3d0-7baed738a857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'labels', 'input_ids'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'labels', 'input_ids'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'labels', 'input_ids'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "F8Ih6UlcvnEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Transformer Model"
      ],
      "metadata": {
        "id": "11JLu4TYsLxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define class"
      ],
      "metadata": {
        "id": "CYQKmA0avPtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "embedding_dim = 128\n",
        "num_attention_heads = 8\n",
        "seq_max_len = 128\n",
        "feedforward_dims = 128\n",
        "num_encoder_layers = 6\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, vocab_size, num_classes):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.positional_embedding = nn.Embedding(seq_max_len, embedding_dim)\n",
        "    self.transformer_encoder = nn.TransformerEncoder(\n",
        "        nn.TransformerEncoderLayer(embedding_dim, num_attention_heads, feedforward_dims),\n",
        "        num_encoder_layers\n",
        "    )\n",
        "    self.linear = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "  def forward(self, src, src_key_padding_mask):\n",
        "    src = self.token_embedding(src) + self.positional_embedding(torch.arange(0, src.size(0)).unsqueeze(1).to(device))\n",
        "    src = src.permute(1, 0, 2)\n",
        "\n",
        "    memory = self.transformer_encoder(src, src_key_padding_mask)\n",
        "    output = self.linear(memory)\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "r4XHon2Usf1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "vocab_size, num_classes = len(word2idx), 9\n",
        "model = Transformer(vocab_size, num_classes)\n",
        "model.to(device)\n",
        "\n",
        "using_loaded_weights = False\n",
        "\n",
        "model_path = './task3.pt'\n",
        "if os.path.exists(model_path):\n",
        "  using_loaded_weights = True\n",
        "  model.load_state_dict(torch.load(model_path))\n",
        "  print(f'Model loaded from {model_path}')\n",
        "\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMZytud9vYMt",
        "outputId": "76bf1439-ad19-49bf-d56b-6c3c95222c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (token_embedding): Embedding(400002, 128)\n",
              "  (positional_embedding): Embedding(128, 128)\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-5): 6 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=128, out_features=9, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build train set"
      ],
      "metadata": {
        "id": "PL6VPapHxH6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = [torch.tensor(item['input_ids']) for item in batch]\n",
        "    labels = [torch.tensor(item['labels']) for item in batch]\n",
        "\n",
        "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)  # Assuming -100 is the ignore index for labels\n",
        "\n",
        "    attn_masks = torch.zeros(input_ids_padded.shape, dtype=torch.long)\n",
        "    attn_masks = input_ids_padded != 0  # 0 where there's padding, 1 elsewhere\n",
        "\n",
        "    max_len = input_ids_padded.size(1)\n",
        "    square_attn_masks = torch.zeros((len(batch), max_len, max_len), dtype=torch.long)\n",
        "    for i, mask in enumerate(attn_masks):\n",
        "        mask_len = mask.sum()\n",
        "        square_attn_masks[i, :mask_len, :mask_len] = 1\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids_padded,\n",
        "        'labels': labels_padded,\n",
        "        'padding_mask': square_attn_masks\n",
        "    }\n"
      ],
      "metadata": {
        "id": "ums81bDXxW5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "shuffle = True\n",
        "\n",
        "train_loader = DataLoader(dataset['train'], batch_size, shuffle, collate_fn=collate_fn)\n",
        "dev_loader = DataLoader(dataset['validation'], batch_size, shuffle, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(dataset['test'], batch_size, shuffle, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "aQxZmdTZxHYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to print green text\n",
        "def print_green(text):\n",
        "  print(f'\\033[92m{text}\\033[0m')"
      ],
      "metadata": {
        "id": "LcUNkXAgNJfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ],
      "metadata": {
        "id": "cFZF5oNWwjQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from conlleval import evaluate\n",
        "\n",
        "def train_model(model):\n",
        "  print('Begin training BiLSTM with GloVe embeddings')\n",
        "\n",
        "  lr = 1e-3\n",
        "  loss_fn = nn.CrossEntropyLoss(ignore_index=9)\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "  tag_to_index = {\n",
        "      'O': 0,\n",
        "      'B-PER': 1,\n",
        "      'I-PER': 2,\n",
        "      'B-ORG': 3,\n",
        "      'I-ORG': 4,\n",
        "      'B-LOC': 5,\n",
        "      'I-LOC': 6,\n",
        "      'B-MISC': 7,\n",
        "      'I-MISC': 8\n",
        "  }\n",
        "  index_to_tag = {index: tag for tag, index in tag_to_index.items()}\n",
        "\n",
        "\n",
        "  num_epochs = 20\n",
        "  for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    train_loss_total = 0\n",
        "    for batch in train_loader:\n",
        "      inputs = batch['input_ids'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "      padding_mask = batch['padding_mask'].to(device)\n",
        "      print(padding_mask.shape)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs, padding_mask)\n",
        "      loss = loss_fn(outputs.permute(0,2,1), labels.long())\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      train_loss_total += loss.item()\n",
        "\n",
        "    train_loss_ave = train_loss_total / len(train_loader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, train loss: {train_loss_ave:.4f}')\n",
        "\n",
        "    # Evaluation phase\n",
        "    model.eval()\n",
        "    dev_loss_total = 0\n",
        "    pred_tags = []\n",
        "    true_tags = []\n",
        "    with torch.no_grad():\n",
        "      for batch in dev_loader:\n",
        "        inputs = batch['input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        padding_mask = batch['padding_mask'].to(device)\n",
        "\n",
        "        outputs = model(inputs, padding_mask)\n",
        "        loss = loss_fn(outputs.permute(0,2,1), labels.long())\n",
        "        dev_loss_total += loss.item()\n",
        "\n",
        "        preds = torch.argmax(outputs, dim=2)\n",
        "        for i in range(labels.size(0)):\n",
        "          pred_seq = preds[i].cpu().numpy()\n",
        "          true_seq = labels[i].cpu().numpy()\n",
        "\n",
        "          indices_valid = true_seq != 9\n",
        "          valid_pred_tags = [index_to_tag[idx] for idx in pred_seq[indices_valid]]\n",
        "          valid_true_tags = [index_to_tag[idx] for idx in true_seq[indices_valid]]\n",
        "\n",
        "          pred_tags.append(valid_pred_tags)\n",
        "          true_tags.append(valid_true_tags)\n",
        "\n",
        "    dev_loss_ave = dev_loss_total / len(dev_loader)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, dev loss: {dev_loss_ave:.4f}')\n",
        "\n",
        "    # Calculate metrics\n",
        "    pred_tags_flattened = []\n",
        "    for valid_pred_tag in pred_tags:\n",
        "      for tag in valid_pred_tag:\n",
        "        pred_tags_flattened.append(tag)\n",
        "\n",
        "    true_tags_flattened = []\n",
        "    for valid_true_tag in true_tags:\n",
        "      for tag in valid_true_tag:\n",
        "        true_tags_flattened.append(tag)\n",
        "\n",
        "    precision, recall, f1 = evaluate(true_tags_flattened, pred_tags_flattened)\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Precision: {precision}, Recall: {recall}, F1: {f1}')\n",
        "\n",
        "    early_stopping_epoch, min_f1 = 10, 77\n",
        "    if epoch >= early_stopping_epoch and f1 >= min_f1:\n",
        "      print_green('Expected F1 reached! ðŸš€ðŸš€'\n",
        "            f'Epoch: {epoch+1}, F1: {f1}')\n",
        "      break"
      ],
      "metadata": {
        "id": "bwI9aobGwkfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model and save weights"
      ],
      "metadata": {
        "id": "niTxuMwKKofX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not using_loaded_weights:\n",
        "  print('Training model...')\n",
        "  train_model(model)\n",
        "  torch.save(model.state_dict(), model_path)\n",
        "else:\n",
        "  print('Using loaded model weights')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "Z5uEoLFaFoZI",
        "outputId": "0c6888fd-3fc1-41c6-c3b7-b79bf6da5ba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model...\n",
            "Begin training BiLSTM with GloVe embeddings\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-303-61c105223431>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0musing_loaded_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-301-462c2431ef72>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtrain_loss_total\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0mpadding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'padding_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate model"
      ],
      "metadata": {
        "id": "m0tRUbNSK7yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, loader, desc):\n",
        "  tag_to_index = {\n",
        "      'O': 0,\n",
        "      'B-PER': 1,\n",
        "      'I-PER': 2,\n",
        "      'B-ORG': 3,\n",
        "      'I-ORG': 4,\n",
        "      'B-LOC': 5,\n",
        "      'I-LOC': 6,\n",
        "      'B-MISC': 7,\n",
        "      'I-MISC': 8\n",
        "  }\n",
        "  index_to_tag = {index: tag for tag, index in tag_to_index.items()}\n",
        "\n",
        "  # Testing phase\n",
        "  model.eval()\n",
        "  pred_tags = []\n",
        "  true_tags = []\n",
        "  with torch.no_grad():\n",
        "    for batch in loader:\n",
        "      inputs = batch['input_ids'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      preds = torch.argmax(outputs, dim=2)\n",
        "      for i in range(labels.size(0)):\n",
        "        pred_seq = preds[i].cpu().numpy()\n",
        "        true_seq = labels[i].cpu().numpy()\n",
        "\n",
        "        indices_valid = true_seq != 9\n",
        "        valid_pred_tags = [index_to_tag[idx] for idx in pred_seq[indices_valid]]\n",
        "        valid_true_tags = [index_to_tag[idx] for idx in true_seq[indices_valid]]\n",
        "\n",
        "        pred_tags.append(valid_pred_tags)\n",
        "        true_tags.append(valid_true_tags)\n",
        "\n",
        "  # Calculate metrics\n",
        "  pred_tags_flattened = []\n",
        "  for valid_pred_tag in pred_tags:\n",
        "    for tag in valid_pred_tag:\n",
        "      pred_tags_flattened.append(tag)\n",
        "\n",
        "  true_tags_flattened = []\n",
        "  for valid_true_tag in true_tags:\n",
        "    for tag in valid_true_tag:\n",
        "      true_tags_flattened.append(tag)\n",
        "\n",
        "  precision, recall, f1 = evaluate(true_tags_flattened, pred_tags_flattened)\n",
        "  print_green(f'{desc} Data:\\n'\n",
        "        f'Precision: {precision}, Recall: {recall}, F1: {f1}')\n",
        "\n",
        "test_model(model, train_loader, 'Train')\n",
        "test_model(model, dev_loader, 'Validation')\n",
        "test_model(model, test_loader, 'Test')"
      ],
      "metadata": {
        "id": "kODtvQpyK_Ob"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}